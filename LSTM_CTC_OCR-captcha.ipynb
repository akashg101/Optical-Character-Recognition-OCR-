{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ga.sharma/anaconda3/envs/ocr2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import h5py\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__\n",
    "\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining different variables\n",
    "data_dir = '../data/captcha/'\n",
    "img_width = 32\n",
    "img_height = 32\n",
    "timesteps_size = 32\n",
    "num_hidden_units = 64\n",
    "num_classes = 63\n",
    "\n",
    "# reading the maps\n",
    "label_cls_name_map = {}\n",
    "label_name_cls_map = {}\n",
    "with open('label_cls_name.json', 'r') as f:\n",
    "    label_cls_name_map = json.loads(f.read())\n",
    "    \n",
    "for k,v in label_cls_name_map.iteritems():\n",
    "    label_name_cls_map[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully !!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"test_targets\": shape (2000,), type \"|S6\">"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train dataset inputs and targets\n",
    "train_inputs = []\n",
    "train_targets = []\n",
    "train_image_names = os.listdir(os.path.join(data_dir,'train_images'))\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'ocr_combined_train_annotations.csv'), \n",
    "                      header=None)\n",
    "for i, train_image_name in enumerate(train_image_names[:5000]):\n",
    "    sys.stdout.write('\\r%d' % i)\n",
    "    full_image_path = os.path.join(os.path.join(data_dir, 'train_images'), train_image_name)\n",
    "    image_np = cv2.imread(full_image_path, 0) #reading the image as 1 channel\n",
    "    image_np = cv2.resize(image_np, (img_width, img_height),interpolation = cv2.INTER_AREA).T\n",
    "    train_inputs.append(image_np)\n",
    "    # get the target\n",
    "    target_ = ''.join(train_df[train_df[0].map(\n",
    "        lambda x: x.split('/')[1])==train_image_name][1].tolist())\n",
    "    train_targets.append(target_)\n",
    "    \n",
    "# test dataset inputs and targets\n",
    "test_inputs = []\n",
    "test_targets = []\n",
    "test_image_names = os.listdir(os.path.join(data_dir,'test_images'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'ocr_combined_test_annotations.csv'), \n",
    "                      header=None)\n",
    "for i, test_image_name in enumerate(test_image_names[:5000]):\n",
    "    sys.stdout.write('\\r%d' % i)\n",
    "    full_image_path = os.path.join(os.path.join(data_dir, 'test_images'), test_image_name)\n",
    "    image_np = cv2.imread(full_image_path, 0)\n",
    "    image_np = cv2.resize(image_np, (img_width, img_height),interpolation = cv2.INTER_AREA).T\n",
    "    test_inputs.append(image_np)\n",
    "    # get the target\n",
    "    target_ = ''.join(test_df[test_df[0].map(\n",
    "        lambda x: x.split('/')[1])==test_image_name][1].tolist())\n",
    "    test_targets.append(target_)\n",
    "\n",
    "train_inputs_ = np.array(train_inputs)\n",
    "train_targets_ = np.array(train_targets)\n",
    "test_inputs_ = np.array(test_inputs)\n",
    "test_targets_ = np.array(test_targets)\n",
    "print '\\rData loaded successfully !!!'\n",
    "\n",
    "h5f = h5py.File('data.h5', 'w')\n",
    "h5f.create_dataset('train_inputs', data=train_inputs)\n",
    "h5f.create_dataset('train_targets', data=train_targets)\n",
    "h5f.create_dataset('test_inputs', data=test_inputs)\n",
    "h5f.create_dataset('test_targets', data=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File('data.h5','r')\n",
    "train_inputs = h5f['train_inputs'][:]\n",
    "train_targets = h5f['train_targets'][:]\n",
    "test_inputs = h5f['test_inputs'][:]\n",
    "test_targets = h5f['test_targets'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "print(train_inputs_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb3b960bad0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_inputs_[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32)\n",
      "(5000,)\n",
      "(2000, 32, 32)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print train_inputs_.shape\n",
    "print train_targets_.shape\n",
    "print test_inputs_.shape\n",
    "print test_targets_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert labels into sparse matrix for CTC Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KvLr8b\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_inputs_[0].T)\n",
    "print train_targets_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_integer(labels):\n",
    "    labels_list = list(labels)\n",
    "    int_labels_list = map(lambda x: int(label_name_cls_map[x]), labels_list)\n",
    "    return int_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    \"\"\"Create a sparse representention of x.\n",
    "    Args:\n",
    "        sequences: a list of lists of type dtype where each element is a sequence\n",
    "    Returns:\n",
    "        A tuple with (indices, values, shape)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n]*len(seq), range(len(seq))))\n",
    "        values.extend(seq)\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1]+1], dtype=np.int64)\n",
    "\n",
    "    return indices, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sparse_tensor(sparse_tensor):\n",
    "    \"\"\"Transform sparse to sequences ids.\"\"\"\n",
    "    decoded_indexes = list()\n",
    "    current_i = 0\n",
    "    current_seq = []\n",
    "    for offset, i_and_index in enumerate(sparse_tensor[0]):\n",
    "        i = i_and_index[0]\n",
    "        if i != current_i:\n",
    "            decoded_indexes.append(current_seq)\n",
    "            current_i = i\n",
    "            current_seq = list()\n",
    "        current_seq.append(offset)\n",
    "    decoded_indexes.append(current_seq)\n",
    "\n",
    "    result = []\n",
    "    for index in decoded_indexes:\n",
    "        ids = [sparse_tensor[1][m] for m in index]\n",
    "        text = ''.join(list(map(id2word, ids)))\n",
    "        result.append(text)\n",
    "    return result\n",
    "    \n",
    "def id2word(idx):\n",
    "    return label_cls_name_map[str(idx)]\n",
    "\n",
    "def hit(text1, text2):\n",
    "    \"\"\"Calculate accuracy of predictive text and target text.\"\"\"\n",
    "    res = []\n",
    "    for idx, words1 in enumerate(text1):\n",
    "        res.append(words1 == text2[idx])\n",
    "    return np.mean(np.asarray(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36, 45, 42, 57, 1, 17]\n"
     ]
    }
   ],
   "source": [
    "train_targets_integer = map(convert_labels_to_integer, train_targets_)\n",
    "test_targets_integer = map(convert_labels_to_integer, test_targets_)\n",
    "\n",
    "print train_targets_integer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 32, 32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL LSTM + CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ocr_checkpoints_lstm/best_validation_681\n",
      "248/250Epoch 1/20000, train_cost = 27.212, train_ler = 0.945, accuracy: 0.0000, time = 250.060\n",
      "248/250Epoch 2/20000, train_cost = 25.519, train_ler = 0.943, accuracy: 0.0000, time = 226.466\n",
      "248/250Epoch 3/20000, train_cost = 25.301, train_ler = 0.943, accuracy: 0.0000, time = 201.924\n",
      "248/250Epoch 4/20000, train_cost = 25.262, train_ler = 0.942, accuracy: 0.0000, time = 168.950\n",
      "248/250Epoch 5/20000, train_cost = 25.204, train_ler = 0.941, accuracy: 0.0000, time = 213.991\n",
      "147/250"
     ]
    }
   ],
   "source": [
    "num_epochs = 20000\n",
    "num_examples = train_inputs_.shape[0]\n",
    "batch_size=20\n",
    "num_batches_per_epoch = num_examples/batch_size\n",
    "num_layers = 3\n",
    "\n",
    "graph  = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # batch_size x step_size x element_size\n",
    "    inputs = tf.placeholder(tf.float32, [None, timesteps_size, img_height])\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "    cells = []\n",
    "    for i in range(num_layers):\n",
    "        cell_ = tf.contrib.rnn.LSTMCell(num_hidden_units)\n",
    "        cells.append(cell_)\n",
    "    stack = tf.contrib.rnn.MultiRNNCell(cells,state_is_tuple=True)\n",
    "    \n",
    "    outputs, _ = tf.nn.dynamic_rnn(stack, inputs, seq_len, dtype=tf.float32)\n",
    "    shape = tf.shape(inputs)\n",
    "    batch_s, max_timesteps = shape[0], shape[1]\n",
    "    # batch_size * timesteps x hidden_layer_size\n",
    "    outputs = tf.reshape(outputs, [-1, num_hidden_units])\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal([num_hidden_units, num_classes], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "    logits = tf.matmul(outputs, W) + b\n",
    "    logits = tf.reshape(logits, [batch_s, -1, num_classes])\n",
    "    logits = tf.transpose(logits, (1, 0, 2))\n",
    "    \n",
    "    loss = tf.nn.ctc_loss(targets, logits, seq_len)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)   \n",
    "    \n",
    "    decoded, _ = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)\n",
    "    ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32), targets))\n",
    "    \n",
    "    session =  tf.Session()\n",
    "\n",
    "    # Initializate the weights and biases\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    save_dir = 'ocr_checkpoints_lstm/'\n",
    "    if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "    save_path = os.path.join(save_dir, 'best_validation_681')\n",
    "    saver.restore(sess=session, save_path=save_path)\n",
    "    \n",
    "    for curr_epoch in range(num_epochs):\n",
    "        train_cost = train_ler = acc = 0\n",
    "        start = time.time()\n",
    "        for batch in range(num_batches_per_epoch-1):\n",
    "            sys.stdout.write('\\r%d/%d'% (batch, num_batches_per_epoch))\n",
    "            sparse_train_targets = sparse_tuple_from(train_targets_integer[batch*batch_size :(batch+1)*batch_size])\n",
    "            feed = {inputs: train_inputs_[batch*batch_size :(batch+1)*batch_size],\n",
    "                    targets: sparse_train_targets,\n",
    "                    seq_len: [img_width]*batch_size}\n",
    "\n",
    "            batch_cost, _ = session.run([cost, optimizer], feed)\n",
    "            train_cost += batch_cost*batch_size\n",
    "            decoded_ = session.run(decoded, feed_dict=feed)\n",
    "            train_ler += session.run(ler, feed_dict=feed)*batch_size\n",
    "            ori = decode_sparse_tensor(sparse_train_targets)\n",
    "            pre = decode_sparse_tensor(decoded_[0])\n",
    "            acc += hit(pre, ori)*batch_size\n",
    "            \n",
    "        train_cost /= num_examples\n",
    "        train_ler /= num_examples\n",
    "        acc /= num_examples\n",
    "        \n",
    "        log = \"Epoch {}/{}, train_cost = {:.3f}, train_ler = {:.3f}, accuracy: {:.4f}, time = {:.3f}\"\n",
    "        print(log.format(curr_epoch+1, num_epochs, train_cost, train_ler, acc, time.time() - start))\n",
    "        save_path = os.path.join(save_dir, 'best_validation_' + str(curr_epoch+1))\n",
    "        saver.save(sess=session, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ocr_checkpoints_lstm/best_validation_796\n",
      "[u'74TW4', u'41Bm', u'45d8M', u'436B4']\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20000\n",
    "num_examples = train_inputs_.shape[0]\n",
    "batch_size=20\n",
    "num_batches_per_epoch = num_examples/batch_size\n",
    "num_layers = 3\n",
    "\n",
    "graph  = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # batch_size x step_size x element_size\n",
    "    inputs = tf.placeholder(tf.float32, [None, timesteps_size, img_height])\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "    cells = []\n",
    "    for i in range(num_layers):\n",
    "        cell_ = tf.contrib.rnn.LSTMCell(num_hidden_units)\n",
    "        cells.append(cell_)\n",
    "    stack = tf.contrib.rnn.MultiRNNCell(cells,state_is_tuple=True)\n",
    "    \n",
    "    outputs, _ = tf.nn.dynamic_rnn(stack, inputs, seq_len, dtype=tf.float32)\n",
    "    shape = tf.shape(inputs)\n",
    "    batch_s, max_timesteps = shape[0], shape[1]\n",
    "    # batch_size * timesteps x hidden_layer_size\n",
    "    outputs = tf.reshape(outputs, [-1, num_hidden_units])\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal([num_hidden_units, num_classes], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "    logits = tf.matmul(outputs, W) + b\n",
    "    logits = tf.reshape(logits, [batch_s, -1, num_classes])\n",
    "    logits = tf.transpose(logits, (1, 0, 2))\n",
    "\n",
    "    \n",
    "    decoded, _ = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)\n",
    "    \n",
    "    session =  tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=session, save_path='ocr_checkpoints_lstm/best_validation_796')\n",
    "    \n",
    "    feed = {inputs: train_inputs_[:4], seq_len: [32,32,32,32]}\n",
    "    decoded_ = session.run(decoded, feed_dict=feed)\n",
    "    pre = decode_sparse_tensor(decoded_[0])\n",
    "    print pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KvLr8b'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gVtIDm'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sUw0DH'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EkBB4F'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4v7kFg'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
