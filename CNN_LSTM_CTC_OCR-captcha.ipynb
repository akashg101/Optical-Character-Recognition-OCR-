{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optical Character Recognition Using DeepLearning\n",
    "\n",
    "Text is everywhere! It is present in PDFs, docs as well as images. There are lots of applications where text data is useful for doing analytics. Such applications include receipts recognition, number plate detection, extracting the latex formulas from the images etc. General Computer Vision can be used for such task but it lacks in accuracy. In order to solve the low accuracy and variance problem, we use the state of the art deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Packages and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully !!!\n",
      "Train input shape:  (100, 80, 300, 1)\n",
      "Train targets shape:  (100,)\n",
      "Test input shape:  (10, 80, 300, 1)\n",
      "Test targets shape:  (10,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import h5py\n",
    "import sys\n",
    "import time\n",
    "from IPython.display import Image\n",
    "tf.__version__\n",
    "\n",
    "# defining different variables\n",
    "data_dir = '../data/captcha/'\n",
    "img_width = 300\n",
    "img_height = 80\n",
    "num_channels=1\n",
    "timesteps_size = img_width\n",
    "num_classes = 63\n",
    "\n",
    "# reading the maps\n",
    "label_cls_name_map = {}\n",
    "label_name_cls_map = {}\n",
    "with open('label_cls_name.json', 'r') as f:\n",
    "    label_cls_name_map = json.loads(f.read())\n",
    "    \n",
    "for k,v in label_cls_name_map.iteritems():\n",
    "    label_name_cls_map[v] = k\n",
    "\n",
    "# train dataset inputs and targets\n",
    "train_inputs = []\n",
    "train_targets = []\n",
    "train_image_names = os.listdir(os.path.join(data_dir,'train_images'))\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'ocr_combined_train_annotations.csv'), \n",
    "                      header=None)\n",
    "for i, train_image_name in enumerate(train_image_names[:100]):\n",
    "    sys.stdout.write('\\r%d' % i)\n",
    "    full_image_path = os.path.join(os.path.join(data_dir, 'train_images'), train_image_name)\n",
    "    image_np = cv2.imread(full_image_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.\n",
    "    image_np = np.reshape(image_np, [img_height, img_width, num_channels])\n",
    "    train_inputs.append(image_np)\n",
    "    # get the target\n",
    "    target_ = ''.join(train_df[train_df[0].map(\n",
    "        lambda x: x.split('/')[1])==train_image_name][1].tolist())\n",
    "    train_targets.append(target_)\n",
    "    \n",
    "# test dataset inputs and targets\n",
    "test_inputs = []\n",
    "test_targets = []\n",
    "test_image_names = os.listdir(os.path.join(data_dir,'test_images'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'ocr_combined_test_annotations.csv'), \n",
    "                      header=None)\n",
    "for i, test_image_name in enumerate(test_image_names[:10]):\n",
    "    sys.stdout.write('\\r%d' % i)\n",
    "    full_image_path = os.path.join(os.path.join(data_dir, 'test_images'), test_image_name)\n",
    "    image_np = cv2.imread(full_image_path, cv2.IMREAD_GRAYSCALE).astype(np.float32) / 255.\n",
    "    image_np = np.reshape(image_np, [img_height, img_width, num_channels])\n",
    "    test_inputs.append(image_np)\n",
    "    # get the target\n",
    "    target_ = ''.join(test_df[test_df[0].map(\n",
    "        lambda x: x.split('/')[1])==test_image_name][1].tolist())\n",
    "    test_targets.append(target_)\n",
    "\n",
    "train_inputs_ = np.array(train_inputs)\n",
    "train_targets_ = np.array(train_targets)\n",
    "test_inputs_ = np.array(test_inputs)\n",
    "test_targets_ = np.array(test_targets)\n",
    "print '\\rData loaded successfully !!!'\n",
    "\n",
    "print 'Train input shape: ', train_inputs_.shape\n",
    "print 'Train targets shape: ', train_targets_.shape\n",
    "print 'Test input shape: ', test_inputs_.shape\n",
    "print 'Test targets shape: ', test_targets_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert labels into sparse matrix for CTC Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_integer(labels):\n",
    "    labels_list = list(labels)\n",
    "    int_labels_list = map(lambda x: int(label_name_cls_map[x]), labels_list)\n",
    "    return int_labels_list\n",
    "\n",
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    \"\"\"Create a sparse representention of x.\n",
    "    Args:\n",
    "        sequences: a list of lists of type dtype where each element is a sequence\n",
    "    Returns:\n",
    "        A tuple with (indices, values, shape)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n]*len(seq), range(len(seq))))\n",
    "        values.extend(seq)\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1]+1], dtype=np.int64)\n",
    "\n",
    "    return indices, values, shape\n",
    "\n",
    "def decode_sparse_tensor(sparse_tensor):\n",
    "    \"\"\"Transform sparse to sequences ids.\"\"\"\n",
    "    decoded_indexes = list()\n",
    "    current_i = 0\n",
    "    current_seq = []\n",
    "    for offset, i_and_index in enumerate(sparse_tensor[0]):\n",
    "        i = i_and_index[0]\n",
    "        if i != current_i:\n",
    "            decoded_indexes.append(current_seq)\n",
    "            current_i = i\n",
    "            current_seq = list()\n",
    "        current_seq.append(offset)\n",
    "    decoded_indexes.append(current_seq)\n",
    "\n",
    "    result = []\n",
    "    for index in decoded_indexes:\n",
    "        ids = [sparse_tensor[1][m] for m in index]\n",
    "        text = ''.join(list(map(id2word, ids)))\n",
    "        result.append(text)\n",
    "    return result\n",
    "    \n",
    "def id2word(idx):\n",
    "    return label_cls_name_map[str(idx)]\n",
    "\n",
    "def hit(text1, text2):\n",
    "    \"\"\"Calculate accuracy of predictive text and target text.\"\"\"\n",
    "    res = []\n",
    "    for idx, words1 in enumerate(text1):\n",
    "        res.append(words1 == text2[idx])\n",
    "    return np.mean(np.asarray(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual text:  3l8734\n",
      "label conversion of text:  [0, 14, 25, 21, 0, 12]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6fc6e4e0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_targets_integer = map(convert_labels_to_integer, train_targets_)\n",
    "test_targets_integer = map(convert_labels_to_integer, test_targets_)\n",
    "\n",
    "plt.imshow(train_inputs_[0,:,:,0])\n",
    "print 'Actual text: ', train_targets_[0]\n",
    "print 'label conversion of text: ', train_targets_integer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "We are using 4 CNN followed by 2 LSTM followed by a CTC layer architecture.\n",
    "\n",
    "![Architecture](images/cnn_lstm_Architecture.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-c4a900332106>:87: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert (cnn_count <= count_, \"cnn_count should be <= {}!\".format(count_))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "feature_h: 40, feature_w: 150, out_channels: 64\n",
      "\n",
      "feature_h: 20, feature_w: 75, out_channels: 128\n",
      "\n",
      "feature_h: 10, feature_w: 38, out_channels: 128\n",
      "\n",
      "feature_h: 5, feature_w: 19, out_channels: 64\n",
      "lstm input shape: [2, 19, 320]\n",
      "48/50Epoch 1/100, train_cost = 27.498, train_ler = 0.952,         accuracy: 0.0000, time = 69.044\n",
      "48/50Epoch 2/100, train_cost = 25.459, train_ler = 0.943,         accuracy: 0.0000, time = 64.723\n",
      "48/50Epoch 3/100, train_cost = 25.288, train_ler = 0.948,         accuracy: 0.0000, time = 63.262\n",
      "48/50Epoch 4/100, train_cost = 25.150, train_ler = 0.935,         accuracy: 0.0000, time = 66.336\n",
      "48/50Epoch 5/100, train_cost = 24.863, train_ler = 0.922,         accuracy: 0.0000, time = 73.037\n",
      "48/50Epoch 6/100, train_cost = 24.646, train_ler = 0.923,         accuracy: 0.0000, time = 78.760\n",
      "48/50Epoch 7/100, train_cost = 24.484, train_ler = 0.908,         accuracy: 0.0000, time = 82.693\n",
      "48/50Epoch 8/100, train_cost = 24.292, train_ler = 0.908,         accuracy: 0.0000, time = 80.118\n",
      "48/50Epoch 9/100, train_cost = 23.960, train_ler = 0.903,         accuracy: 0.0000, time = 77.595\n",
      "48/50Epoch 10/100, train_cost = 23.592, train_ler = 0.907,         accuracy: 0.0000, time = 75.257\n",
      "13/50"
     ]
    }
   ],
   "source": [
    "def _conv2d(x, name, filter_size, in_channels, out_channels, strides):\n",
    "        with tf.variable_scope(name):\n",
    "            kernel = tf.get_variable(name='W',\n",
    "                                     shape=[filter_size, filter_size, in_channels,\n",
    "                                            out_channels],\n",
    "                                     dtype=tf.float32,\n",
    "                                     initializer=tf.glorot_uniform_initializer()\n",
    "                                    )  # tf.glorot_normal_initializer\n",
    "\n",
    "            b = tf.get_variable(name='b',\n",
    "                                shape=[out_channels],\n",
    "                                dtype=tf.float32,\n",
    "                                initializer=tf.constant_initializer())\n",
    "\n",
    "            con2d_op = tf.nn.conv2d(x, kernel, [1, strides, strides, 1], padding='SAME')\n",
    "\n",
    "        return tf.nn.bias_add(con2d_op, b)\n",
    "\n",
    "def _batch_norm(name, x):\n",
    "        \"\"\"Batch normalization.\"\"\"\n",
    "        with tf.variable_scope(name):\n",
    "            x_bn = \\\n",
    "                tf.contrib.layers.batch_norm(\n",
    "                    inputs=x,\n",
    "                    decay=0.9,\n",
    "                    center=True,\n",
    "                    scale=True,\n",
    "                    epsilon=1e-5,\n",
    "                    updates_collections=None,\n",
    "                    is_training=mode == 'train',\n",
    "                    fused=True,\n",
    "                    data_format='NHWC',\n",
    "                    zero_debias_moving_mean=True,\n",
    "                    scope='BatchNorm'\n",
    "                )\n",
    "\n",
    "        return x_bn\n",
    "\n",
    "def _leaky_relu(x, leakiness=0.0):\n",
    "        return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')\n",
    "\n",
    "def _max_pool(x, ksize, strides):\n",
    "        return tf.nn.max_pool(x,\n",
    "                              ksize=[1, ksize, ksize, 1],\n",
    "                              strides=[1, strides, strides, 1],\n",
    "                              padding='SAME',\n",
    "                              name='max_pool')\n",
    "\n",
    "    \n",
    "mode='train'\n",
    "num_epochs = 100\n",
    "num_examples = train_inputs_.shape[0]\n",
    "img_height = train_inputs_.shape[1]\n",
    "img_width = train_inputs_.shape[2]\n",
    "num_channels = train_inputs_.shape[3]\n",
    "batch_size=2\n",
    "num_batches_per_epoch = num_examples/batch_size\n",
    "num_layers = 2\n",
    "tf.reset_default_graph()\n",
    "graph  = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # __init__\n",
    "    inputs = tf.placeholder(tf.float32, [None, img_height, timesteps_size, num_channels])\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "    _extra_train_ops = []\n",
    "    \n",
    "    # CNN\n",
    "    out_channels = 64\n",
    "    leakiness = 0.01\n",
    "    output_keep_prob = 0.8\n",
    "    num_hidden = 280\n",
    "    decay_rate= 0.98\n",
    "    decay_steps = 10000\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    filters = [1, 64, 128, 128, out_channels]\n",
    "    strides = [1, 2]\n",
    "    feature_h = img_height\n",
    "    feature_w = img_width\n",
    "    count_ = 0\n",
    "    cnn_count= 4\n",
    "    initial_learning_rate = 1e-3\n",
    "    min_size = min(img_height, img_width)\n",
    "    while min_size > 1:\n",
    "        min_size = (min_size + 1) // 2\n",
    "        count_ += 1\n",
    "    assert (cnn_count <= count_, \"cnn_count should be <= {}!\".format(count_))\n",
    "    \n",
    "    with tf.variable_scope('cnn'):\n",
    "        x = inputs\n",
    "        for i in range(cnn_count):\n",
    "            with tf.variable_scope('unit-%d' % (i + 1)):\n",
    "                x = _conv2d(x, 'cnn-%d' % (i + 1), 3, filters[i], filters[i + 1],\n",
    "                            strides[0])\n",
    "                x = _batch_norm('bn%d' % (i + 1), x)\n",
    "                x = _leaky_relu(x, leakiness)\n",
    "                x = _max_pool(x, 2, strides[1])\n",
    "                _, feature_h, feature_w, out_channels = x.get_shape().as_list()\n",
    "            print('\\nfeature_h: {}, feature_w: {}, out_channels: {}'\n",
    "              .format(feature_h, feature_w, out_channels))\n",
    "                    \n",
    "    #LSTM\n",
    "    with tf.variable_scope('lstm'):\n",
    "        x = tf.transpose(x, [0,2,1,3])\n",
    "        x = tf.reshape(x, [batch_size, feature_w, feature_h * out_channels])\n",
    "        print('lstm input shape: {}'.format(x.get_shape().as_list()))\n",
    "        seq_len = tf.fill([x.get_shape().as_list()[0]], feature_w)\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(num_hidden, state_is_tuple=True)\n",
    "        if mode == 'train':\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell,\n",
    "                                                     output_keep_prob=output_keep_prob)\n",
    "                \n",
    "        cell1 = tf.nn.rnn_cell.LSTMCell(num_hidden, state_is_tuple=True)\n",
    "        if mode == 'train':\n",
    "            cell1 = tf.nn.rnn_cell.DropoutWrapper(cell=cell1, \n",
    "                                                  output_keep_prob=output_keep_prob)\n",
    "        # Stacking rnn cells\n",
    "        stack = tf.nn.rnn_cell.MultiRNNCell([cell, cell1], state_is_tuple=True)\n",
    "        initial_state = stack.zero_state(batch_size, dtype=tf.float32)\n",
    "    \n",
    "    outputs, _ = tf.nn.dynamic_rnn(\n",
    "                cell=stack,\n",
    "                inputs=x,\n",
    "                sequence_length=seq_len,\n",
    "                initial_state=initial_state,\n",
    "                dtype=tf.float32,\n",
    "                time_major=False\n",
    "            )  # [batch_size, max_stepsize, FLAGS.num_hidden]\n",
    "\n",
    "    # Reshaping to apply the same weights over the timesteps\n",
    "    outputs = tf.reshape(outputs, [-1, num_hidden]) \n",
    "    W = tf.get_variable(name='W_out',\n",
    "                            shape=[num_hidden, num_classes],\n",
    "                            dtype=tf.float32,\n",
    "                            initializer=tf.glorot_uniform_initializer())\n",
    "    b = tf.get_variable(name='b_out',\n",
    "                                shape=[num_classes],\n",
    "                                dtype=tf.float32,\n",
    "                                initializer=tf.constant_initializer())\n",
    "\n",
    "    logits = tf.matmul(outputs, W) + b\n",
    "    \n",
    "    shape = tf.shape(x)\n",
    "    logits = tf.reshape(logits, [shape[0], -1, num_classes])\n",
    "    logits = tf.transpose(logits, (1, 0, 2))\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    loss = tf.nn.ctc_loss(labels=targets, inputs=logits, sequence_length=seq_len)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    tf.summary.scalar('cost', cost)\n",
    "\n",
    "    lrn_rate = tf.train.exponential_decay(initial_learning_rate,\n",
    "                                                   global_step,\n",
    "                                                   decay_steps,\n",
    "                                                   decay_rate,\n",
    "                                                   staircase=True)\n",
    "    tf.summary.scalar('learning_rate', lrn_rate)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lrn_rate, \n",
    "                                       beta1=beta1,\n",
    "                                        beta2=beta2).minimize(cost,\n",
    "                                                              global_step=global_step)   \n",
    "    train_ops = [optimizer] + _extra_train_ops\n",
    "    train_op = tf.group(*train_ops)\n",
    "    \n",
    "    decoded, log_prob = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)\n",
    "    #decoded, self.log_prob = tf.nn.ctc_greedy_decoder(logits, seq_len,merge_repeated=False)\n",
    "    dense_decoded = tf.sparse_tensor_to_dense(decoded[0], default_value=-1)\n",
    "    ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32), targets))\n",
    "    merged_summay = tf.summary.merge_all()\n",
    "    \n",
    "    #decoded, _ = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)\n",
    "    \n",
    "    session =  tf.Session()\n",
    "\n",
    "    # Initializate the weights and biases\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    save_dir = 'ocr_checkpoints_cnn_lstm/'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    #saver.restore(sess=session, save_path='ocr_checkpoints_lstm/best_validation_300')\n",
    "    \n",
    "    for curr_epoch in range(num_epochs):\n",
    "        train_cost = train_ler = acc = 0\n",
    "        start = time.time()\n",
    "        for batch in range(num_batches_per_epoch-1):\n",
    "            sys.stdout.write('\\r%d/%d'% (batch, num_batches_per_epoch))\n",
    "            sparse_train_targets = sparse_tuple_from(train_targets_integer\n",
    "                                                     [batch*batch_size :\n",
    "                                                      (batch+1)*batch_size])\n",
    "            feed = {inputs: train_inputs_[batch*batch_size :(batch+1)*batch_size],\n",
    "                    targets: sparse_train_targets}\n",
    "\n",
    "            batch_cost, _ = session.run([cost, optimizer], feed)\n",
    "            train_cost += batch_cost*batch_size\n",
    "            decoded_ = session.run(decoded, feed_dict=feed)\n",
    "            train_ler += session.run(ler, feed_dict=feed)*batch_size\n",
    "            ori = decode_sparse_tensor(sparse_train_targets)\n",
    "            #print 'ori:', ori\n",
    "            pre = decode_sparse_tensor(decoded_[0])\n",
    "            #print 'pre:', pre\n",
    "            acc += hit(pre, ori)*batch_size\n",
    "            \n",
    "        train_cost /= num_examples\n",
    "        train_ler /= num_examples\n",
    "        acc /= num_examples\n",
    "        \n",
    "        log = \"Epoch {}/{}, train_cost = {:.3f}, train_ler = {:.3f}, \\\n",
    "        accuracy: {:.4f}, time = {:.3f}\"\n",
    "        print(log.format(curr_epoch+1, num_epochs, train_cost, train_ler, \n",
    "                         acc, time.time() - start))\n",
    "        save_path = os.path.join(save_dir, 'best_validation_' + str(curr_epoch+1))\n",
    "        saver.save(sess=session, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
