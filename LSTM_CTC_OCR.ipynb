{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ga.sharma/anaconda3/envs/ocr2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import h5py\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining different variables\n",
    "data_dir = '../data/NIST19_combined/'\n",
    "img_width = 140\n",
    "img_height = 28\n",
    "timesteps_size = 140\n",
    "num_hidden_units = 256\n",
    "num_classes = 63\n",
    "\n",
    "# reading the maps\n",
    "label_cls_name_map = {}\n",
    "label_name_cls_map = {}\n",
    "with open('label_cls_name.json', 'r') as f:\n",
    "    label_cls_name_map = json.loads(f.read())\n",
    "    \n",
    "for k,v in label_cls_name_map.iteritems():\n",
    "    label_name_cls_map[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully !!!\n"
     ]
    }
   ],
   "source": [
    "# train dataset inputs and targets\n",
    "train_inputs = []\n",
    "train_targets = []\n",
    "train_image_names = os.listdir(os.path.join(data_dir,'train_images'))\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'ocr_combined_train_annotations.csv'))\n",
    "for train_image_name in train_image_names:\n",
    "    full_image_path = os.path.join(os.path.join(data_dir, 'train_images'), train_image_name)\n",
    "    image_np = cv2.imread(full_image_path, 0) # reading the image as 1 channel\n",
    "    image_np = cv2.resize(image_np, (img_width, img_height), interpolation = cv2.INTER_AREA).T\n",
    "    train_inputs.append(image_np)\n",
    "    # get the target\n",
    "    target_ = ''.join(train_df[train_df['filename']==train_image_name]['class'].tolist())\n",
    "    train_targets.append(target_)\n",
    "    \n",
    "# test dataset inputs and targets\n",
    "test_inputs = []\n",
    "test_targets = []\n",
    "test_image_names = os.listdir(os.path.join(data_dir,'test_images'))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, 'ocr_combined_test_annotations.csv'))\n",
    "for test_image_name in test_image_names:\n",
    "    full_image_path = os.path.join(os.path.join(data_dir, 'test_images'), test_image_name)\n",
    "    image_np = cv2.imread(full_image_path, 0)\n",
    "    image_np = cv2.resize(image_np, (img_width, img_height), interpolation = cv2.INTER_AREA).T\n",
    "    test_inputs.append(image_np)\n",
    "    # get the target\n",
    "    target_ = ''.join(test_df[test_df['filename']==test_image_name]['class'].tolist())\n",
    "    test_targets.append(target_)\n",
    "\n",
    "train_inputs_ = np.array(train_inputs)\n",
    "train_targets_ = np.array(train_targets)\n",
    "test_inputs_ = np.array(test_inputs)\n",
    "test_targets_ = np.array(test_targets)\n",
    "print 'Data loaded successfully !!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 140, 28)\n"
     ]
    }
   ],
   "source": [
    "print(train_inputs_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8abbbebe50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_inputs_[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 140, 28)\n",
      "(20000,)\n",
      "(2000, 140, 28)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print train_inputs_.shape\n",
    "print train_targets_.shape\n",
    "print test_inputs_.shape\n",
    "print test_targets_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert labels into sparse matrix for CTC Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ld7a9KL9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_inputs_[0].T)\n",
    "print train_targets_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_integer(labels):\n",
    "    labels_list = list(labels)\n",
    "    int_labels_list = map(lambda x: int(label_name_cls_map[x]), labels_list)\n",
    "    return int_labels_list\n",
    "\n",
    "train_targets_integer = map(convert_labels_to_integer, train_targets_)\n",
    "test_targets_integer = map(convert_labels_to_integer, test_targets_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    \"\"\"Create a sparse representention of x.\n",
    "    Args:\n",
    "        sequences: a list of lists of type dtype where each element is a sequence\n",
    "    Returns:\n",
    "        A tuple with (indices, values, shape)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n]*len(seq), range(len(seq))))\n",
    "        values.extend(seq)\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1]+1], dtype=np.int64)\n",
    "\n",
    "    return indices, values, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sparse_tensor(sparse_tensor):\n",
    "    \"\"\"Transform sparse to sequences ids.\"\"\"\n",
    "    decoded_indexes = list()\n",
    "    current_i = 0\n",
    "    current_seq = []\n",
    "    for offset, i_and_index in enumerate(sparse_tensor[0]):\n",
    "        i = i_and_index[0]\n",
    "        if i != current_i:\n",
    "            decoded_indexes.append(current_seq)\n",
    "            current_i = i\n",
    "            current_seq = list()\n",
    "        current_seq.append(offset)\n",
    "    decoded_indexes.append(current_seq)\n",
    "\n",
    "    result = []\n",
    "    for index in decoded_indexes:\n",
    "        ids = [sparse_tensor[1][m] for m in index]\n",
    "        text = ''.join(list(map(id2word, ids)))\n",
    "        result.append(text)\n",
    "    return result\n",
    "    \n",
    "def id2word(idx):\n",
    "    return str(idx)\n",
    "\n",
    "def hit(text1, text2):\n",
    "    \"\"\"Calculate accuracy of predictive text and target text.\"\"\"\n",
    "    res = []\n",
    "    for idx, words1 in enumerate(text1):\n",
    "        res.append(words1 == text2[idx])\n",
    "    return np.mean(np.asarray(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 30, 15, 0, 52, 36, 42, 52]\n"
     ]
    }
   ],
   "source": [
    "print train_targets_integer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_len = [timesteps_size]*train_inputs_.shape[0] \n",
    "test_seq_len = [timesteps_size]*test_inputs_.shape[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL LSTM + CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/400"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-22ae9e3ac5d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mtrain_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_cost\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mdecoded_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mtrain_ler\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mori\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_train_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mpre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ga.sharma/anaconda3/envs/ocr2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ga.sharma/anaconda3/envs/ocr2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ga.sharma/anaconda3/envs/ocr2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ga.sharma/anaconda3/envs/ocr2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ga.sharma/anaconda3/envs/ocr2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "num_examples = train_inputs_.shape[0]\n",
    "batch_size=50\n",
    "num_batches_per_epoch = num_examples/batch_size\n",
    "num_layers = 2\n",
    "\n",
    "graph  = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # batch_size x step_size x element_size\n",
    "    inputs = tf.placeholder(tf.float32, [None, timesteps_size, img_height])\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "    cells = []\n",
    "    for i in range(num_layers):\n",
    "        cell_ = tf.contrib.rnn.LSTMCell(num_hidden_units)\n",
    "        cells.append(cell_)\n",
    "    stack = tf.contrib.rnn.MultiRNNCell(cells,state_is_tuple=True)\n",
    "    \n",
    "    outputs, _ = tf.nn.dynamic_rnn(stack, inputs, seq_len, dtype=tf.float32)\n",
    "    shape = tf.shape(inputs)\n",
    "    batch_s, max_timesteps = shape[0], shape[1]\n",
    "    # batch_size * timesteps x hidden_layer_size\n",
    "    outputs = tf.reshape(outputs, [-1, num_hidden_units])\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal([num_hidden_units, num_classes], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "    logits = tf.matmul(outputs, W) + b\n",
    "    logits = tf.reshape(logits, [batch_s, -1, num_classes])\n",
    "    logits = tf.transpose(logits, (1, 0, 2))\n",
    "    \n",
    "    loss = tf.nn.ctc_loss(targets, logits, seq_len)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)   \n",
    "    \n",
    "    decoded, _ = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)\n",
    "    ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32), targets))\n",
    "    \n",
    "    session =  tf.Session()\n",
    "\n",
    "    # Initializate the weights and biases\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for curr_epoch in range(num_epochs):\n",
    "        train_cost = train_ler = acc = 0\n",
    "        start = time.time()\n",
    "        for batch in range(num_batches_per_epoch-1):\n",
    "            sys.stdout.write('\\r%d/%d'% (batch, num_batches_per_epoch))\n",
    "            sparse_train_targets = sparse_tuple_from(train_targets_integer[batch*batch_size :(batch+1)*batch_size])\n",
    "            feed = {inputs: train_inputs_[batch*batch_size :(batch+1)*batch_size],\n",
    "                    targets: sparse_train_targets,\n",
    "                    seq_len: train_seq_len[batch*batch_size :(batch+1)*batch_size]}\n",
    "\n",
    "            batch_cost, _ = session.run([cost, optimizer], feed)\n",
    "            train_cost += batch_cost*batch_size\n",
    "            decoded_ = session.run(decoded, feed_dict=feed)\n",
    "            train_ler += session.run(ler, feed_dict=feed)*batch_size\n",
    "            ori = decode_sparse_tensor(sparse_train_targets)\n",
    "            pre = decode_sparse_tensor(decoded_[0])\n",
    "            acc += hit(pre, ori)*batch_size\n",
    "            \n",
    "        train_cost /= num_examples\n",
    "        train_ler /= num_examples\n",
    "        acc /= num_examples\n",
    "        \n",
    "        log = \"Epoch {}/{}, train_cost = {:.3f}, train_ler = {:.3f}, accuracy: {:.4f}, time = {:.3f}\"\n",
    "        print(log.format(curr_epoch+1, num_epochs, train_cost, train_ler, acc, time.time() - start))\n",
    "        saver = tf.train.Saver()\n",
    "        save_dir = 'ocr_checkpoints_lstm/'\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        save_path = os.path.join(save_dir, 'best_validation_' + str(curr_epoch+1))\n",
    "        saver.save(sess=session, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ocr_checkpoints_lstm/best_validation_49\n",
      "['3830154952484252', '596137101340']\n"
     ]
    }
   ],
   "source": [
    "graph  = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # batch_size x step_size x element_size\n",
    "    inputs = tf.placeholder(tf.float32, [None, timesteps_size, img_height])\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "    cells = []\n",
    "    for i in range(num_layers):\n",
    "        cell_ = tf.contrib.rnn.LSTMCell(num_hidden_units)\n",
    "        cells.append(cell_)\n",
    "    stack = tf.contrib.rnn.MultiRNNCell(cells,state_is_tuple=True)\n",
    "    \n",
    "    outputs, _ = tf.nn.dynamic_rnn(stack, inputs, seq_len, dtype=tf.float32)\n",
    "    shape = tf.shape(inputs)\n",
    "    batch_s, max_timesteps = shape[0], shape[1]\n",
    "    # batch_size * timesteps x hidden_layer_size\n",
    "    outputs = tf.reshape(outputs, [-1, num_hidden_units])\n",
    "    \n",
    "    W = tf.Variable(tf.truncated_normal([num_hidden_units, num_classes], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "    logits = tf.matmul(outputs, W) + b\n",
    "    logits = tf.reshape(logits, [batch_s, -1, num_classes])\n",
    "    logits = tf.transpose(logits, (1, 0, 2))\n",
    "\n",
    "    \n",
    "    decoded, _ = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)\n",
    "    \n",
    "    session =  tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess=session, save_path='ocr_checkpoints_lstm/best_validation_49')\n",
    "    \n",
    "    feed = {inputs: train_inputs_[0:2], seq_len: train_seq_len[0:2]}\n",
    "    decoded_ = session.run(decoded, feed_dict=feed)\n",
    "    pre = decode_sparse_tensor(decoded_[0])\n",
    "    print pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_inputs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
